{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "deadly-direction",
   "metadata": {},
   "source": [
    "## Python Modules for Web Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "approved-stability",
   "metadata": {},
   "source": [
    "Web scraping is the process of constructing an agent which can extract, parse, download and organize useful information from the web automatically. In other words, instead of manually saving the data from websites, the web scraping software will automatically load and extract data from multiple websites as per our requirement.\n",
    "\n",
    "In this section, we are going to discuss about useful Python libraries for web scraping.\n",
    "\n",
    "The first question to ask before getting started with any python application is ‘Which libraries do I need?\n",
    "\n",
    "`For web scraping there are a few different libraries to consider, including:`\n",
    "\n",
    "- Beautiful Soup\n",
    "- Requests\n",
    "- Scrapy\n",
    "- Selenium\n",
    "- Urllib3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enormous-title",
   "metadata": {},
   "source": [
    "### Requests\n",
    "It is a simple python web scraping library. It is an efficient HTTP library used for accessing web pages. With the help of Requests, we can get the raw HTML of web pages which can then be parsed for retrieving the data. Before using requests, let us understand its installation.\n",
    "\n",
    "#### Installation of Requests\n",
    "To install Requests, simply run this simple command in your terminal of choice:\n",
    "\n",
    "`pip install requests`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "absolute-danish",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\nijat\\anaconda3\\lib\\site-packages (2.25.1)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\nijat\\anaconda3\\lib\\site-packages (from requests) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\nijat\\anaconda3\\lib\\site-packages (from requests) (2021.5.30)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\nijat\\anaconda3\\lib\\site-packages (from requests) (4.0.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\nijat\\anaconda3\\lib\\site-packages (from requests) (1.26.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excellent-latino",
   "metadata": {},
   "source": [
    "### Example\n",
    "In this example, we are making a GET HTTP request for a web page. For this we need to first import requests library as follows −\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fabulous-singing",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "buried-circulation",
   "metadata": {},
   "source": [
    "In this following line of code, we use requests to make a GET HTTP requests for the url: https://authoraditiagarwal.com/ by making a GET request.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "frequent-darwin",
   "metadata": {},
   "outputs": [],
   "source": [
    "url  = 'https://authoraditiagarwal.com/'\n",
    "response = requests.get(url)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eight-roads",
   "metadata": {},
   "source": [
    "Now we can retrieve the content by using .text property as follows −"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "correct-credits",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<!DOCTYPE html><html lang=\"en-US\" id=\"html\"><head><meta charset=\"UTF-8\" /><meta http-equiv=\"X-UA-Compatible\" content=\"IE=10\" /><link rel=\"profile\" hre'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.text[:150]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "insured-safety",
   "metadata": {},
   "source": [
    "Requests allows you to send HTTP/1.1 requests extremely easily. There’s no need to manually add query strings to your URLs, or to form-encode your POST data. Keep-alive and HTTP connection pooling are 100% automatic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pressing-badge",
   "metadata": {},
   "source": [
    "We will disscuse this later it was just  an intruduction to Request library.\n",
    "\n",
    "https://docs.python-requests.org/en/master/user/quickstart/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "welcome-housing",
   "metadata": {},
   "source": [
    "## Urllib3\n",
    "It is another Python library that can be used for retrieving data from URLs similar to the requests library. You can read more on this at its technical documentation at \n",
    "<a href='https://urllib3.readthedocs.io/en/latest/'>here</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "processed-roller",
   "metadata": {},
   "source": [
    "#### Installing\n",
    "urllib3 can be installed with <a href = 'https://pip.pypa.io/'>pip</a>.\n",
    "\n",
    "`$ python -m pip install urllib3`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "technical-monroe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: urllib3 in c:\\users\\nijat\\anaconda3\\lib\\site-packages (1.26.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install urllib3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thrown-campaign",
   "metadata": {},
   "source": [
    "#### Example: Scraping using Urllib3 and BeautifulSoup\n",
    "\n",
    "In the following example, we are scraping the web page by using Urllib3 and BeautifulSoup. We are using Urllib3 at the place of requests library for getting the raw data (HTML) from web page. Then we are using BeautifulSoup for parsing that HTML data.\n",
    "\n",
    "Beautiful Soup is a Python package for parsing HTML and XML documents (including having malformed markup, i.e. non-closed tags, so named after tag soup). It creates a parse tree for parsed pages that can be used to extract data from HTML, which is useful for web scraping.\n",
    "\n",
    "if you want to know more about the Beautiful Soup please refer to this <a href = 'https://www.crummy.com/software/BeautifulSoup/bs4/doc/'>here</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "faced-alcohol",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<title>Learn and grow together</title>\n",
      "Learn and grow together\n"
     ]
    }
   ],
   "source": [
    "import urllib3\n",
    "from bs4 import BeautifulSoup\n",
    "http = urllib3.PoolManager()\n",
    "response = http.request('GET', 'https://authoraditiagarwal.com')\n",
    "soup = BeautifulSoup(response.data, 'lxml')\n",
    "print (soup.title)\n",
    "print (soup.title.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compound-omaha",
   "metadata": {},
   "source": [
    "### Selenium\n",
    "It is an open source automated testing suite for web applications across different browsers and platforms. It is not a single tool but a suite of software. We have selenium bindings for Python, Java, C#, Ruby and JavaScript. Here we are going to perform web scraping by using selenium and its Python bindings. \n",
    "\n",
    "Selenium Python bindings provide a convenient API to access Selenium WebDrivers like Firefox, IE, Chrome, Remote etc. The current supported Python versions are 2.7, 3.5 and above.\n",
    "\n",
    "for doecumentaion please visit here https://pypi.org/project/selenium/.\n",
    "\n",
    "#### Installing Selenium\n",
    "\n",
    "Using the pip command, we can install urllib3 either in our virtual environment or in global installation.\n",
    "\n",
    "`pip install selenium`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "specific-purchase",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting selenium\n",
      "  Downloading selenium-3.141.0-py2.py3-none-any.whl (904 kB)\n",
      "Requirement already satisfied: urllib3 in c:\\users\\nijat\\anaconda3\\lib\\site-packages (from selenium) (1.26.4)\n",
      "Installing collected packages: selenium\n",
      "Successfully installed selenium-3.141.0\n"
     ]
    }
   ],
   "source": [
    "!pip install selenium"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cooperative-fraction",
   "metadata": {},
   "source": [
    "As selenium requires a driver to interface with the chosen browser, we need to download it. The following table shows different browsers and their links for downloading the same.\n",
    "\n",
    "- Chrome https://sites.google.com/a/chromium.org/\n",
    "\n",
    "- Edge https://developer.microsoft.com/\n",
    "\n",
    "- Firefox https://github.com/\n",
    "\n",
    "- Safari https://webkit.org/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surface-break",
   "metadata": {},
   "source": [
    "#### Example\n",
    "\n",
    "This example shows web scraping using selenium. It can also be used for testing which is called selenium testing.\n",
    "\n",
    "After downloading the particular driver for the specified version of browser, we need to do programming in Python.\n",
    "\n",
    "First, need to import webdriver from selenium as follows −\n",
    "\n",
    "`from selenium import webdriver`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "engaged-craft",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "radio-colonial",
   "metadata": {},
   "source": [
    "Now, provide the path of web driver which we have downloaded as per our requirement −\n",
    "\n",
    "`path = r'C:\\Users\\nijat\\Desktop\\Data Science\\Preparation For Interview\\Technicl Skil\\web Scraping\\Chromedriver'`<br>\n",
    "`browser = webdriver.Chrome(executable_path = path)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "deadly-migration",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r'C:\\Users\\nijat\\Desktop\\Data Science\\Preparation For Interview\\Technicl Skil\\web Scraping\\Chromedriver'\n",
    "browser = webdriver.Chrome(executable_path = path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "medical-antibody",
   "metadata": {},
   "source": [
    "Now, provide the url which we want to open in that web browser now controlled by our Python script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "preceding-cradle",
   "metadata": {},
   "outputs": [],
   "source": [
    "browser.get('https://authoraditiagarwal.com/leadershipmanagement')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opponent-strike",
   "metadata": {},
   "source": [
    "We can also scrape a particular element by providing the xpath as provided in lxml."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "criminal-liabilities",
   "metadata": {},
   "outputs": [],
   "source": [
    "browser.find_element_by_xpath('/html/body').click()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "valid-geneva",
   "metadata": {},
   "source": [
    "You can check the browser, controlled by Python script, for output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "active-prototype",
   "metadata": {},
   "source": [
    "## Scrapy\n",
    "Scrapy is a fast, open-source web crawling framework written in Python, used to extract the data from the web page with the help of selectors based on XPath. Scrapy was first released on June 26, 2008 licensed under BSD, with a milestone 1.0 releasing in June 2015. It provides us all the tools we need to extract, process and structure the data from websites.\n",
    "\n",
    "if you want to know more about the Scapy visit <a herf=\"https://docs.scrapy.org/en/latest/\">here</a>\n",
    "\n",
    "#### Installing Scrapy\n",
    "Using the pip command, we can install urllib3 either in our virtual environment or in global installation.\n",
    "\n",
    "`pip install scrapy`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "emotional-flight",
   "metadata": {},
   "source": [
    "We will be using `Beautiful Soup`. But before to do web scraping Let's disscuse some important issue.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "august-hello",
   "metadata": {},
   "source": [
    "## Legality of Web Scraping\n",
    "\n",
    "With Python, we can scrape any website or particular elements of a web page but do you have any idea whether it is legal or not? Before scraping any website we must have to know about the legality of web scraping. This chapter will explain the concepts related to legality of web scraping.\n",
    "\n",
    "Generally, if you are going to use the scraped data for personal use, then there may not be any problem. But if you are going to republish that data, then before doing the same you should make download request to the owner or do some background research about policies as well about the data you are going to scrape.\n",
    "\n",
    "### Research Required Prior to Scraping\n",
    "If you are targeting a website for scraping data from it, we need to understand its scale and structure. Following are some of the files which we need to analyze before starting web scraping.\n",
    "\n",
    "#### 1. Analyzing robots.txt\n",
    "Actually most of the publishers allow programmers to crawl their websites at some extent. In other sense, publishers want specific portions of the websites to be crawled. To define this, websites must put some rules for stating which portions can be crawled and which cannot be. Such rules are defined in a file called robots.txt.\n",
    "\n",
    "robots.txt is human readable file used to identify the portions of the website that crawlers are allowed as well as not allowed to scrape. There is no standard format of robots.txt file and the publishers of website can do modifications as per their needs. We can check the robots.txt file for a particular website by providing a slash and robots.txt after url of that website. For example, if we want to check it for Google.com, then we need to type https://www.google.com/robots.txt and we will get something as follows −\n",
    "<code>\n",
    "User-agent: *\n",
    "Disallow: /search\n",
    "Allow: /search/about\n",
    "Allow: /search/static\n",
    "Allow: /search/howsearchworks\n",
    "Disallow: /sdch\n",
    "Disallow: /groups\n",
    "Disallow: /index.html?\n",
    "Disallow: /?\n",
    "Allow: /?hl=\n",
    "Disallow: /?hl=*&\n",
    "Allow: /?hl=*&gws_rd=ssl$\n",
    "and so on……..\n",
    "</code>\n",
    "\n",
    "#### 2. Analyzing Sitemap files\n",
    "What you supposed to do if you want to crawl a website for updated information? You will crawl every web page for getting that updated information, but this will increase the server traffic of that particular website. That is why websites provide sitemap files for helping the crawlers to locate updating content without needing to crawl every web page. Sitemap standard is defined at http://www.sitemaps.org/protocol.html.\n",
    "\n",
    "\n",
    "#### 3. What is the Size of Website?\n",
    "Is the size of a website, i.e. the number of web pages of a website affects the way we crawl? Certainly yes. Because if we have less number of web pages to crawl, then the efficiency would not be a serious issue, but suppose if our website has millions of web pages, for example Microsoft.com, then downloading each web page sequentially would take several months and then efficiency would be a serious concern.\n",
    "\n",
    "#### 4. Checking Website’s Size\n",
    "By checking the size of result of Google’s crawler, we can have an estimate of the size of a website. Our result can be filtered by using the keyword site while doing the Google search. For example, estimating the size of https://authoraditiagarwal.com/ is given below −\n",
    "\n",
    "<img src = 'https://www.tutorialspoint.com/python_web_scraping/images/checking_the_size.jpg'>\n",
    "\n",
    "\n",
    "You can see there are around 60 results which mean it is not a big website and crawling would not lead the efficiency issue."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cubic-consumer",
   "metadata": {},
   "source": [
    "#### 5. Which technology is used by website?\n",
    "Another important question is whether the technology used by website affects the way we crawl? Yes, it affects. But how we can check about the technology used by a website? There is a Python library named builtwith with the help of which we can find out about the technology used by a website.\n",
    "\n",
    "##### Example\n",
    "\n",
    "In this example we are going to check the technology used by the website https://authoraditiagarwal.com with the help of Python library builtwith. But before using this library, we need to install it as follows −"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "specific-ordinance",
   "metadata": {},
   "source": [
    "Let's first install  builtwith\n",
    "\n",
    "`pip install builtwith`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "renewable-attribute",
   "metadata": {},
   "source": [
    "Now with the help of following simple line of codes we can check the technology used by a particular website −"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "powerful-sudan",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'web-servers': ['Apache'],\n",
       " 'advertising-networks': ['Google AdSense'],\n",
       " 'javascript-frameworks': ['Prototype', 'jQuery'],\n",
       " 'ecommerce': ['WooCommerce'],\n",
       " 'cms': ['WordPress'],\n",
       " 'programming-languages': ['PHP'],\n",
       " 'blogs': ['PHP', 'WordPress']}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import builtwith\n",
    "builtwith.parse('http://authoraditiagarwal.com')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "encouraging-bloom",
   "metadata": {},
   "source": [
    "#### 6. Who is the owner of website?\n",
    "\n",
    "The owner of the website also matters because if the owner is known for blocking the crawlers, then the crawlers must be careful while scraping the data from website. There is a protocol named Whois with the help of which we can find out about the owner of the website.\n",
    "\n",
    "Let's first install the whois \n",
    "\n",
    "`pip install python-whois`\n",
    "\n",
    "#### Example\n",
    "\n",
    "In this example we are going to check the owner of the website say `microsoft.com` with the help of Whois. But before using this library, we need to install it as follows −"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "objective-restriction",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"domain_name\": [\n",
      "    \"MICROSOFT.COM\",\n",
      "    \"microsoft.com\"\n",
      "  ],\n",
      "  \"registrar\": \"MarkMonitor, Inc.\",\n",
      "  \"whois_server\": \"whois.markmonitor.com\",\n",
      "  \"referral_url\": null,\n",
      "  \"updated_date\": [\n",
      "    \"2021-03-12 23:25:32\",\n",
      "    \"2021-04-07 12:58:15\"\n",
      "  ],\n",
      "  \"creation_date\": [\n",
      "    \"1991-05-02 04:00:00\",\n",
      "    \"1991-05-01 21:00:00\"\n",
      "  ],\n",
      "  \"expiration_date\": [\n",
      "    \"2022-05-03 04:00:00\",\n",
      "    \"2022-05-02 00:00:00\"\n",
      "  ],\n",
      "  \"name_servers\": [\n",
      "    \"NS1-205.AZURE-DNS.COM\",\n",
      "    \"NS2-205.AZURE-DNS.NET\",\n",
      "    \"NS3-205.AZURE-DNS.ORG\",\n",
      "    \"NS4-205.AZURE-DNS.INFO\",\n",
      "    \"ns1-205.azure-dns.com\",\n",
      "    \"ns2-205.azure-dns.net\",\n",
      "    \"ns4-205.azure-dns.info\",\n",
      "    \"ns3-205.azure-dns.org\"\n",
      "  ],\n",
      "  \"status\": [\n",
      "    \"clientDeleteProhibited https://icann.org/epp#clientDeleteProhibited\",\n",
      "    \"clientTransferProhibited https://icann.org/epp#clientTransferProhibited\",\n",
      "    \"clientUpdateProhibited https://icann.org/epp#clientUpdateProhibited\",\n",
      "    \"serverDeleteProhibited https://icann.org/epp#serverDeleteProhibited\",\n",
      "    \"serverTransferProhibited https://icann.org/epp#serverTransferProhibited\",\n",
      "    \"serverUpdateProhibited https://icann.org/epp#serverUpdateProhibited\",\n",
      "    \"clientUpdateProhibited (https://www.icann.org/epp#clientUpdateProhibited)\",\n",
      "    \"clientTransferProhibited (https://www.icann.org/epp#clientTransferProhibited)\",\n",
      "    \"clientDeleteProhibited (https://www.icann.org/epp#clientDeleteProhibited)\",\n",
      "    \"serverUpdateProhibited (https://www.icann.org/epp#serverUpdateProhibited)\",\n",
      "    \"serverTransferProhibited (https://www.icann.org/epp#serverTransferProhibited)\",\n",
      "    \"serverDeleteProhibited (https://www.icann.org/epp#serverDeleteProhibited)\"\n",
      "  ],\n",
      "  \"emails\": [\n",
      "    \"abusecomplaints@markmonitor.com\",\n",
      "    \"admin@domains.microsoft\",\n",
      "    \"msnhst@microsoft.com\",\n",
      "    \"whoisrequest@markmonitor.com\"\n",
      "  ],\n",
      "  \"dnssec\": \"unsigned\",\n",
      "  \"name\": \"Domain Administrator\",\n",
      "  \"org\": \"Microsoft Corporation\",\n",
      "  \"address\": \"One Microsoft Way,\",\n",
      "  \"city\": \"Redmond\",\n",
      "  \"state\": \"WA\",\n",
      "  \"zipcode\": \"98052\",\n",
      "  \"country\": \"US\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import whois\n",
    "print (whois.whois('microsoft.com'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "concrete-evening",
   "metadata": {},
   "source": [
    "### Data Extraction\n",
    "\n",
    "Analyzing a web page means understanding its sructure . Now, the question arises why it is important for web scraping?\n",
    "\n",
    "#### Web page Analysis\n",
    "Web page analysis is important because without analyzing we are not able to know in which form we are going to receive the data from (structured or unstructured) that web page after extraction. We can do web page analysis in the following ways −\n",
    "\n",
    "#### Viewing Page Source\n",
    "\n",
    "This is a way to understand how a web page is structured by examining its source code. To implement this, we need to right click the page and then must select the View page source option. Then, we will get the data of our interest from that web page in the form of HTML. But the main concern is about whitespaces and formatting which is difficult for us to format.\n",
    "\n",
    "#### Inspecting Page Source by Clicking Inspect Element Option\n",
    "\n",
    "This is another way of analyzing web page. But the difference is that it will resolve the issue of formatting and whitespaces in the source code of web page. You can implement this by right clicking and then selecting the Inspect or Inspect element option from menu. It will provide the information about particular area or element of that web page.\n",
    "\n",
    "### Different Ways to Extract Data from Web Page\n",
    "\n",
    "The following methods are mostly used for extracting data from a web page −\n",
    "\n",
    "#### 1. Regular Expression\n",
    "They are highly specialized programming language embedded in Python. We can use it through re module of Python. It is also called RE or regexes or regex patterns. With the help of regular expressions, we can specify some rules for the possible set of strings we want to match from the data.\n",
    "\n",
    "If you want to learn more about regular expression in general, go to the link https://www.tutorialspoint.com/automata_theory/regular_expressions.htm and if you want to know more about re module or regular expression in Python, you can follow the link https://www.tutorialspoint.com/python/python_reg_expressions.htm.\n",
    "\n",
    "#### Example\n",
    "\n",
    "In the following example, we are going to scrape data about India from http://example.webscraping.com after matching the contents of `<td>` with the help of regular expression.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "absent-adapter",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import urllib.request\n",
    "\n",
    "response = urllib.request.urlopen('http://example.webscraping.com/places/default/view/India-102')\n",
    "\n",
    "html = response.read()\n",
    "text = html.decode()\n",
    "re.findall('<td class=\"w2p_fw\">(.*?)</td>',text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "impressive-resistance",
   "metadata": {},
   "source": [
    "### Beautiful Soup\n",
    "\n",
    "Suppose we want to collect all the hyperlinks from a web page, then we can use a parser called BeautifulSoup which can be known in more detail at https://www.crummy.com/software/BeautifulSoup/bs4/doc/. In simple words, BeautifulSoup is a Python library for pulling data out of HTML and XML files. It can be used with requests, because it needs an input (document or url) to create a soup object asit cannot fetch a web page by itself. You can use the following Python script to gather the title of web page and hyperlinks.\n",
    "\n",
    "#### Installing Beautiful Soup\n",
    "\n",
    "Using the pip command, we can install beautifulsoup either in our virtual environment or in global installation.\n",
    "\n",
    "`pip install bs4`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "modified-fireplace",
   "metadata": {},
   "source": [
    "#### Example\n",
    "Note that in this example, we are extending the above example implemented with requests python module. we are using r.text for creating a soup object which will further be used to fetch details like title of the webpage.\n",
    "\n",
    "First, we need to import necessary Python modules −\n",
    "<code>\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "legal-parker",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests \n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "offshore-wayne",
   "metadata": {},
   "source": [
    "In this following line of code we use requests to make a GET HTTP requests for the url: https://authoraditiagarwal.com/ by making a GET request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "official-reunion",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get('https://authoraditiagarwal.com/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "protecting-teacher",
   "metadata": {},
   "source": [
    "Now we need to create a Soup object as follows −\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "twelve-learning",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<title>Learn and grow together</title>\n",
      "Learn and grow together\n"
     ]
    }
   ],
   "source": [
    "soup = BeautifulSoup(r.text, 'lxml')\n",
    "print (soup.title)\n",
    "print (soup.title.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "natural-earthquake",
   "metadata": {},
   "source": [
    "### Lxml\n",
    "\n",
    "Another Python library we are going to discuss for web scraping is lxml. It is a highperformance HTML and XML parsing library. It is comparatively fast and straightforward. You can read about it more on https://lxml.de/.\n",
    "\n",
    "##### Installing lxml\n",
    "\n",
    "`pip install lxml`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "incident-proxy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lxml in c:\\users\\nijat\\anaconda3\\lib\\site-packages (4.6.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install lxml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "animated-visiting",
   "metadata": {},
   "source": [
    "Now we need to provide the url of web page to scrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "outdoor-request",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://authoraditiagarwal.com/leadershipmanagement/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enhanced-alias",
   "metadata": {},
   "source": [
    "Now we need to provide the path (Xpath) to particular element of that web page −"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "outer-agriculture",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-fbbaee94c57d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0msource_code\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhtml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfromstring\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbyte_string\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mtree\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msource_code\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxpath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtree\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext_content\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from lxml import html \n",
    "path = '//*[@id=\"panel-836-0-0-1\"]/div/div/p[1]'\n",
    "response = requests.get(url)\n",
    "byte_string = response.content\n",
    "source_code = html.fromstring(byte_string)\n",
    "tree = source_code.xpath(path)\n",
    "print(tree[0].text_content()) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "whole-survival",
   "metadata": {},
   "source": [
    "## Data Processing\n",
    "\n",
    "To process the data that has been scraped, we must store the data on our local machine in a particular format like spreadsheet (CSV), JSON or sometimes in databases like MySQL.\n",
    "\n",
    "### 1. CSV and JSON Data Processing\n",
    "First, we are going to write the information, after grabbing from web page, into a CSV file or a spreadsheet. Let us first understand through a simple example in which we will first grab the information using `BeautifulSoup` module, as did earlier, and then by using Python CSV module we will write that textual information into CSV file.\n",
    "\n",
    "First, we need to import the necessary Python libraries as follows −"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "sublime-paradise",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "literary-tract",
   "metadata": {},
   "source": [
    "In this following line of code, we use requests to make a GET HTTP requests for the url: https://authoraditiagarwal.com/ by making a GET request.\n",
    "\n",
    "` r = requests.get('https://authoraditiagarwal.com/')` \n",
    "\n",
    "Now, we need to create a Soup object as follows −\n",
    "\n",
    "`soup = BeautifulSoup(r.text, 'lxml')`\n",
    "\n",
    "Now, with the help of next lines of code, we will write the grabbed data into a CSV file named dataprocessing.csv.\n",
    "<code>\n",
    "f = csv.writer(open(' dataprocessing.csv ','w'))\n",
    "f.writerow(['Title'])\n",
    "f.writerow([soup.title.text])\n",
    "</code>\n",
    "\n",
    "Now let's put all together and run the script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "recent-prison",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "r = requests.get('https://authoraditiagarwal.com/')\n",
    "soup = BeautifulSoup(r.text,'lxml')\n",
    "f = csv.writer(open('dataprocessing.csv','r+'))\n",
    "f.writerow(['Title'])\n",
    "f.writerow([soup.title.text])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "static-evening",
   "metadata": {},
   "source": [
    "After running this script, the textual information or the title of the webpage will be saved in the above mentioned CSV file on your local machine."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continent-confirmation",
   "metadata": {},
   "source": [
    "Let's read csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "explicit-vulnerability",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Learn and grow together</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Title\n",
       "0  Learn and grow together"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('dataprocessing.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coordinate-monte",
   "metadata": {},
   "source": [
    "Similarly, we can save the collected information in a JSON file. The following is an easy to understand Python script for doing the same in which we are grabbing the same information as we did in last Python script, but this time the grabbed information is saved in JSONfile.txt by using JSON Python module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "reduced-franchise",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import json\n",
    "r = requests.get('https://authoraditiagarwal.com/')\n",
    "soup = BeautifulSoup(r.text, 'lxml')\n",
    "y = json.dumps(soup.title.text)\n",
    "with open('JSONFile.json', 'wt') as outfile:\n",
    "   json.dump(y, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "magnetic-priest",
   "metadata": {},
   "source": [
    "Let's read Json file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ultimate-alfred",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"Learn and grow together\"'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "  \n",
    "# Opening JSON file\n",
    "f = open('JSONFile.json',)\n",
    "  \n",
    "# returns JSON object as \n",
    "# a dictionary\n",
    "data = json.load(f)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wrong-marijuana",
   "metadata": {},
   "source": [
    "After running this script, the grabbed information i.e. title of the webpage will be saved in the above mentioned text file on your local machine."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extra-relations",
   "metadata": {},
   "source": [
    "## Data Processing using AWS S3\n",
    "\n",
    "to understand this visit this url.  https://www.tutorialspoint.com/python_web_scraping/python_web_scraping_data_processing.htm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minimal-drink",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "southwest-retirement",
   "metadata": {},
   "source": [
    "## Data processing using MySQL\n",
    "\n",
    "With the help of following steps, we can scrape and process data into MySQL table −\n",
    "- Step 1 − First, by using MySQL we need to create a database and table in which we want to save our scraped data. For example, we are creating the table with following query −\n",
    "<code>\n",
    "CREATE TABLE Scrap_pages (id BIGINT(7) NOT NULL AUTO_INCREMENT,\n",
    "title VARCHAR(200), content VARCHAR(10000),PRIMARY KEY(id));\n",
    "</code>\n",
    "\n",
    "- Step 2 − Next, we need to deal with Unicode. Note that MySQL does not handle Unicode by default. We need to turn on this feature with the help of following commands which will change the default character set for the database, for the table and for both of the columns −\n",
    "\n",
    "<code>\n",
    "ALTER DATABASE scrap CHARACTER SET = utf8mb4 COLLATE = utf8mb4_unicode_ci;\n",
    "ALTER TABLE Scrap_pages CONVERT TO CHARACTER SET utf8mb4 COLLATE\n",
    "utf8mb4_unicode_ci;\n",
    "ALTER TABLE Scrap_pages CHANGE title title VARCHAR(200) CHARACTER SET utf8mb4\n",
    "COLLATE utf8mb4_unicode_ci;\n",
    "ALTER TABLE pages CHANGE content content VARCHAR(10000) CHARACTER SET utf8mb4\n",
    "COLLATE utf8mb4_unicode_ci;\n",
    "</code>\n",
    "\n",
    "- Step 3 − Now, integrate MySQL with Python. For this, we will need PyMySQL which can be installed with the help of the following command\n",
    "\n",
    "`pip install PyMySQL `\n",
    "\n",
    "Step 4 − Now, our database named Scrap, created earlier, is ready to save the data, after scraped from web, into table named Scrap_pages. Here in our example we are going to scrape data from Wikipedia and it will be saved into our database.\n",
    "\n",
    "First, we need to import the required Python modules.\n",
    "\n",
    "<code>\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import datetime\n",
    "import random\n",
    "import pymysql\n",
    "import re\n",
    "</code>\n",
    "\n",
    "Now, make a connection, that is integrate this with Python.\n",
    "\n",
    "<code>\n",
    "conn = pymysql.connect(host='127.0.0.1',user='root', passwd = None, db = 'mysql',\n",
    "charset = 'utf8')\n",
    "cur = conn.cursor()\n",
    "cur.execute(\"USE scrap\")\n",
    "random.seed(datetime.datetime.now())\n",
    "def store(title, content):\n",
    "   cur.execute('INSERT INTO scrap_pages (title, content) VALUES ''(\"%s\",\"%s\")', (title, content))\n",
    "   cur.connection.commit()\n",
    "</code>\n",
    "\n",
    "Now, connect with Wikipedia and get data from it.\n",
    "\n",
    "<code>\n",
    "def getLinks(articleUrl):\n",
    "   html = urlopen('http://en.wikipedia.org'+articleUrl)\n",
    "   bs = BeautifulSoup(html, 'html.parser')\n",
    "   title = bs.find('h1').get_text()\n",
    "   content = bs.find('div', {'id':'mw-content-text'}).find('p').get_text()\n",
    "   store(title, content)\n",
    "   return bs.find('div', {'id':'bodyContent'}).findAll('a',href=re.compile('^(/wiki/)((?!:).)*$'))\n",
    "links = getLinks('/wiki/Kevin_Bacon')\n",
    "try:\n",
    "   while len(links) > 0:\n",
    "      newArticle = links[random.randint(0, len(links)-1)].attrs['href']\n",
    "      print(newArticle)\n",
    "      links = getLinks(newArticle)\n",
    "</code>\n",
    "\n",
    "Lastly, we need to close both cursor and connection.\n",
    "\n",
    "<code>\n",
    "finally:\n",
    "   cur.close()\n",
    "   conn.close()\n",
    "</code>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "legislative-poetry",
   "metadata": {},
   "source": [
    "## Processing Images and Videos\n",
    "\n",
    "The web media content that we obtain during scraping can be images, audio and video files, in the form of non-web pages as well as data files. But, can we trust the downloaded data especially on the extension of data we are going to download and store in our computer memory? This makes it essential to know about the type of data we are going to store locally.\n",
    "\n",
    "### Getting Media Content from Web Page\n",
    "\n",
    "we are going to learn how we can download media content which correctly represents the media type based on the information from web server. We can do it with the help of Python requests module as we did in previous chapter.\n",
    "\n",
    "First, we need to import necessary Python modules as follows −"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "clean-laptop",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "steady-blake",
   "metadata": {},
   "source": [
    "Now, provide the URL of the media content we want to download and store locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "seven-commercial",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://cdn.shortpixel.ai/client/to_avif,q_lossy,ret_img,w_300/https://authoraditiagarwal.com/wp-content/uploads/2020/06/evonne-yuwen-teoh-KYmH8ZqKjJ4-unsplash-copy-300x200.jpg\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "artistic-classic",
   "metadata": {},
   "source": [
    "Use the following code to create HTTP response object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "whole-flush",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get(url) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "resident-tournament",
   "metadata": {},
   "source": [
    "With the help of following line of code, we can save the received content as .png file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "physical-sucking",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"ThinkBig.png\",'wb') as f:\n",
    "   f.write(r.content) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continuous-elevation",
   "metadata": {},
   "source": [
    "let's check the image. \n",
    "\n",
    "<img src = 'ThinkBig.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fifty-powder",
   "metadata": {},
   "source": [
    "After running the above Python script, we will get a file named ThinkBig.png, which would have the downloaded image."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "answering-columbia",
   "metadata": {},
   "source": [
    "## Extracting Filename from URL\n",
    "\n",
    "After downloading the content from web site, we also want to save it in a file with a file name found in the URL. But we can also check, if numbers of additional fragments exist in URL too. For this, we need to find the actual filename from the URL.\n",
    "\n",
    "With the help of following Python script, using urlparse, we can extract the filename from URL −\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "twenty-roulette",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/wpcontent/uploads/2018/05/MetaSlider_ThinkBig-1080x180.jpg'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import urllib3\n",
    "from urllib.parse import urlparse\n",
    "import os\n",
    "url = \"https://authoraditiagarwal.com/wpcontent/uploads/2018/05/MetaSlider_ThinkBig-1080x180.jpg\"\n",
    "a = urlparse(url)\n",
    "a.path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coordinated-innocent",
   "metadata": {},
   "source": [
    "### Information about Type of Content from URL\n",
    "\n",
    "While extracting the contents from web server, by GET request, we can also check its information provided by the web server. With the help of following Python script we can determine what web server means with the type of the content −\n",
    "\n",
    "First, we need to import necessary Python modules as follows −\n",
    "\n",
    "`import requests`\n",
    "\n",
    "Now, we need to provide the URL of the media content we want to download and store locally.\n",
    "\n",
    "`url = \"https://authoraditiagarwal.com/wpcontent/uploads/2018/05/MetaSlider_ThinkBig-1080x180.jpg\"`\n",
    "\n",
    "Following line of code will create HTTP response object.\n",
    "\n",
    "`r = requests.get(url, allow_redirects=True)`\n",
    "\n",
    "Now, we can get what type of information about content can be provided by web server.\n",
    "\n",
    "`for headers in r.headers: print(headers)`\n",
    "\n",
    "Let's put all together.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "illegal-samba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date\n",
      "Content-Type\n",
      "Content-Length\n",
      "Connection\n",
      "Server\n",
      "CDN-PullZone\n",
      "CDN-Uid\n",
      "CDN-RequestCountryCode\n",
      "Access-Control-Allow-Origin\n",
      "Access-Control-Allow-Headers\n",
      "Access-Control-Expose-Headers\n",
      "CDN-EdgeStorageId\n",
      "Link\n",
      "X-Tag\n",
      "Pragma\n",
      "Expires\n",
      "Cache-Control\n",
      "Last-Modified\n",
      "CDN-CachedAt\n",
      "CDN-RequestPullSuccess\n",
      "CDN-RequestPullCode\n",
      "CDN-RequestId\n",
      "CDN-Cache\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "url = \"https://cdn.shortpixel.ai/client/to_avif,q_lossy,ret_img,w_300/https://authoraditiagarwal.com/wp-content/uploads/2020/06/evonne-yuwen-teoh-KYmH8ZqKjJ4-unsplash-copy-300x200.jpg\"\n",
    "r = requests.get(url, allow_redirects=True)\n",
    "for headers in r.headers:\n",
    "    print(headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "integrated-brazilian",
   "metadata": {},
   "source": [
    "With the help of following line of code we can get the particular information about content type, say content-type −"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "simplified-romance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image/jpeg\n"
     ]
    }
   ],
   "source": [
    "print (r.headers.get('content-type'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sharp-latin",
   "metadata": {},
   "source": [
    "With the help of following line of code, we can get the particular information about content type, say EType −"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "dying-quebec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print (r.headers.get('ETag'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "tribal-federation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11462\n"
     ]
    }
   ],
   "source": [
    "print (r.headers.get('content-length'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conservative-masters",
   "metadata": {},
   "source": [
    "With the help of following line of code we can get the particular information about content type, say Server −"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "perceived-clock",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BunnyCDN-MU1-675\n"
     ]
    }
   ],
   "source": [
    "print (r.headers.get('Server'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mobile-circular",
   "metadata": {},
   "source": [
    "## Generating Thumbnail for Images\n",
    "Thumbnail is a very small description or representation. A user may want to save only thumbnail of a large image or save both the image as well as thumbnail. In this section we are going to create a thumbnail of the image named ThinkBig.png downloaded in the previous section “Getting media content from web page”.\n",
    "\n",
    "For this Python script, we need to install Python library named Pillow, a fork of the Python Image library having useful functions for manipulating images. It can be installed with the help of following command −\n",
    "\n",
    "`pip install pillow`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ceramic-jumping",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pillow in c:\\users\\nijat\\anaconda3\\lib\\site-packages (8.0.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pillow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hired-border",
   "metadata": {},
   "source": [
    "The following Python script will create a thumbnail of the image and will save it to the current directory by prefixing thumbnail file with Th_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "final-oracle",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from PIL import Image\n",
    "for infile in glob.glob(\"ThinkBig.png\"):\n",
    "   img = Image.open(infile)\n",
    "   img.thumbnail((128, 128), Image.ANTIALIAS)\n",
    "   if infile[0:2] != \"Th_\":\n",
    "      img.save(\"Th_\" + infile, \"png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "backed-thermal",
   "metadata": {},
   "source": [
    "let's read see the Th_ThinkBig.png\n",
    "\n",
    "\n",
    "<img src = 'Th_ThinkBig.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "protecting-breathing",
   "metadata": {},
   "source": [
    "### Screenshot from Website\n",
    "\n",
    "In web scraping, a very common task is to take screenshot of a website. For implementing this, we are going to use selenium and webdriver. The following Python script will take the screenshot from website and will save it to current directory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "tribal-trinity",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method WebDriver.quit of <selenium.webdriver.chrome.webdriver.WebDriver (session=\"bd023a55b59487c1283c1d019728bff1\")>>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "path = r'C:\\Users\\nijat\\Desktop\\Data Science\\Preparation For Interview\\Technicl Skil\\web Scraping\\Chromedriver'\n",
    "browser = webdriver.Chrome(executable_path = path)\n",
    "browser.get('https://www.linkedin.com/in/nijatullah-mansoor-276976199/')\n",
    "screenshot = browser.save_screenshot('screenshot.png')\n",
    "browser.quit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unnecessary-landing",
   "metadata": {},
   "source": [
    "Now let's read see the screenshoot \n",
    "\n",
    "<img src= 'screenshot.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "southwest-vessel",
   "metadata": {},
   "source": [
    "### Thumbnail Generation for Video\n",
    "\n",
    "Suppose we have downloaded videos from website and wanted to generate thumbnails for them so that a specific video, based on its thumbnail, can be clicked. For generating thumbnail for videos we need a simple tool called ffmpeg which can be downloaded from www.ffmpeg.org. After downloading, we need to install it as per the specifications of our OS.\n",
    "\n",
    "The following Python script will generate thumbnail of the video and will save it to our local directory\n",
    "\n",
    "<code>\n",
    "import subprocess\n",
    "video_MP4_file = “C:\\Users\\gaurav\\desktop\\solar.mp4\n",
    "thumbnail_image_file = 'thumbnail_solar_video.jpg'\n",
    "subprocess.call(['ffmpeg', '-i', video_MP4_file, '-ss', '00:00:20.000', '-\n",
    "   vframes', '1', thumbnail_image_file, \"-y\"]) \n",
    "</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mechanical-syracuse",
   "metadata": {},
   "source": [
    "### Ripping an MP4 video to an MP3\n",
    "\n",
    "Suppose you have downloaded some video file from a website, but you only need audio from that file to serve your purpose, then it can be done in Python with the help of Python library called moviepy which can be installed with the help of following command −\n",
    "\n",
    "`pip install moviepy`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beautiful-wellington",
   "metadata": {},
   "source": [
    "Now, after successfully installing moviepy with the help of following script we can convert and MP4 to MP3.\n",
    "\n",
    "<code>\n",
    "import moviepy.editor as mp\n",
    "clip = mp.VideoFileClip(r\"C:\\Users\\gaurav\\Desktop\\1234.mp4\")\n",
    "clip.audio.write_audiofile(\"movie_audio.mp3\")\n",
    "</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "strong-coral",
   "metadata": {},
   "source": [
    "# Dealing with Text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confused-coordination",
   "metadata": {},
   "source": [
    "You can perform text analysis in by using Python library called Natural Language Tool Kit (NLTK). Before proceeding into the concepts of NLTK, let us understand the relation between text analysis and web scraping.\n",
    "\n",
    "Analyzing the words in the text can lead us to know about which words are important, which words are unusual, how words are grouped. This analysis eases the task of web scraping.\n",
    "\n",
    "#### Getting started with NLTK\n",
    "The Natural language toolkit (NLTK) is collection of Python libraries which is designed especially for identifying and tagging parts of speech found in the text of natural language like English.\n",
    "\n",
    "#### Installing NLTK\n",
    "You can use the following command to install NLTK in Python −\n",
    "\n",
    "`pip install nltk`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "chinese-glossary",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\nijat\\anaconda3\\lib\\site-packages (3.5)\n",
      "Requirement already satisfied: click in c:\\users\\nijat\\anaconda3\\lib\\site-packages (from nltk) (7.1.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\nijat\\anaconda3\\lib\\site-packages (from nltk) (4.59.0)\n",
      "Requirement already satisfied: joblib in c:\\users\\nijat\\anaconda3\\lib\\site-packages (from nltk) (1.0.1)\n",
      "Requirement already satisfied: regex in c:\\users\\nijat\\anaconda3\\lib\\site-packages (from nltk) (2021.3.17)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "severe-television",
   "metadata": {},
   "source": [
    "##### Downloading NLTK’s Data\n",
    "After installing NLTK, we have to download preset text repositories. But before downloading text preset repositories, we need to import NLTK with the help of import command as follows −\n",
    "\n",
    "`import nltk`\n",
    "\n",
    "Now, with the help of following command NLTK data can be downloaded −\n",
    "\n",
    "`nltk.download()`\n",
    "\n",
    "Installation of all available packages of NLTK will take some time, but it is always recommended to install all the packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "necessary-salmon",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "typical-bahrain",
   "metadata": {},
   "source": [
    "#### Installing Other Necessary packages\n",
    "\n",
    "We also need some other Python packages like gensim and pattern for doing text analysis as well as building building natural language processing applications by using NLTK.\n",
    "\n",
    "`gensim` − A robust semantic modeling library which is useful for many applications. It can be installed by the following command −\n",
    "\n",
    "`pip install gensim`\n",
    "\n",
    "`pattern` − Used to make gensim package work properly. It can be installed by the following command −\n",
    "\n",
    "`pip install pattern`\n",
    "\n",
    "#### Tokenization\n",
    "\n",
    "The Process of breaking the given text, into the smaller units called tokens, is called tokenization. These tokens can be the words, numbers or punctuation marks. It is also called word segmentation.\n",
    "\n",
    "<img src='https://www.tutorialspoint.com/python_web_scraping/images/tokenization.jpg'>\n",
    "\n",
    "NLTK module provides different packages for tokenization. We can use these packages as per our requirement. Some of the packages are described here −\n",
    "\n",
    "`sent_tokenize package `− This package will divide the input text into sentences. You can use the following command to import this package −\n",
    "\n",
    "`from nltk.tokenize import sent_tokenize`\n",
    "\n",
    "`word_tokenize package` − This package will divide the input text into words. You can use the following command to import this package −\n",
    "\n",
    "`from nltk.tokenize import word_tokenize`\n",
    "\n",
    "`WordPunctTokenizer package` − This package will divide the input text as well as the punctuation marks into words. You can use the following command to import this package −\n",
    "\n",
    "`from nltk.tokenize import WordPuncttokenizer`\n",
    "\n",
    "\n",
    "### Stemming\n",
    "In any language, there are different forms of a words. A language includes lots of variations due to the grammatical reasons. For example, consider the words `democracy`, `democratic`, and `democratization`. For machine learning as well as for web scraping projects, it is important for machines to understand that these different words have the same base form.` Hence we can say that it can be useful to extract the base forms of the words while analyzing the text.`\n",
    "\n",
    "This can be `achieved by stemming` which may be defined as the heuristic process of extracting the base forms of the words by chopping off the ends of words.\n",
    "\n",
    "NLTK module provides different packages for stemming. We can use these packages as per our requirement. Some of these packages are described here −\n",
    "\n",
    "`PorterStemmer package `− Porter’s algorithm is used by this Python stemming package to extract the base form. You can use the following command to import this package −\n",
    "\n",
    "`from nltk.tokenize import WordPuncttokenizer`\n",
    "\n",
    "For example, after giving the word ‘`writing`’ as the input to this stemmer, the output would be the word ‘`write’` after stemming.\n",
    "\n",
    "`LancasterStemmer package` − Lancaster’s algorithm is used by this Python stemming package to extract the base form. You can use the following command to import this package −\n",
    "\n",
    "`from nltk.stem.lancaster import LancasterStemmer`\n",
    "\n",
    "For example, after giving the word ‘writing’ as the input to this stemmer then the output would be the word ‘writ’ after stemming.\n",
    "\n",
    "`SnowballStemmer package` − Snowball’s algorithm is used by this Python stemming package to extract the base form. You can use the following command to import this package −\n",
    "\n",
    "`from nltk.stem.snowball import SnowballStemmer`\n",
    "\n",
    "For example, after giving the word ‘writing’ as the input to this stemmer then the output would be the word ‘write’ after stemming."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rubber-hunter",
   "metadata": {},
   "source": [
    "#### Lemmatization\n",
    "An other way to extract the base form of words is by lemmatization, normally aiming to remove inflectional endings by using vocabulary and morphological analysis. The base form of any word after lemmatization is called lemma.\n",
    "\n",
    "NLTK module provides following packages for lemmatization −\n",
    "\n",
    "`WordNetLemmatizer package` − It will extract the base form of the word depending upon whether it is used as noun as a verb. You can use the following command to import this package −\n",
    "\n",
    "`from nltk.stem import WordNetLemmatizer`\n",
    "\n",
    "### Chunking\n",
    "Chunking, which means `dividing` the data into `small chunks`, is one of the important processes in natural language processing to identify the parts of speech and short phrases like noun phrases. Chunking is to do the labeling of tokens. We can get the structure of the sentence with the help of chunking process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "provincial-growth",
   "metadata": {},
   "source": [
    "### Example\n",
    "In this example, we are going to implement Noun-Phrase chunking by using NLTK Python module. NP chunking is a category of chunking which will find the noun phrases chunks in the sentence.\n",
    "\n",
    "Steps for implementing noun phrase chunking\n",
    "We need to follow the steps given below for implementing noun-phrase chunking −\n",
    "\n",
    "Step 1 − Chunk grammar definition\n",
    "\n",
    "In the first step we will define the grammar for chunking. It would consist of the rules which we need to follow.\n",
    "\n",
    "Step 2 − Chunk parser creation\n",
    "\n",
    "Now, we will create a chunk parser. It would parse the grammar and give the output.\n",
    "\n",
    "Step 3 − The Output\n",
    "\n",
    "In this last step, the output would be produced in a tree format.\n",
    "\n",
    "First, we need to import the NLTK package as follows −"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "local-stable",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "motivated-letters",
   "metadata": {},
   "source": [
    "Next, we need to define the sentence. Here DT: the determinant, VBP: the verb, JJ: the adjective, IN: the preposition and NN: the noun."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "removed-saturday",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = [(\"a\", \"DT\"),(\"clever\",\"JJ\"),(\"fox\",\"NN\"),(\"was\",\"VBP\"),(\"jumping\",\"VBP\"),(\"over\",\"IN\"),(\"the\",\"DT\"),(\"wall\",\"NN\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intense-implementation",
   "metadata": {},
   "source": [
    "Next, we are giving the grammar in the form of regular expression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "australian-difference",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NP:{<DT>?<JJ>*<NN>}'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grammar = \"NP:{<DT>?<JJ>*<NN>}\"\n",
    "grammar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nonprofit-thermal",
   "metadata": {},
   "source": [
    "Now, next line of code will define a parser for parsing the grammar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "lined-sunglasses",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser_chunking = nltk.RegexpParser(grammar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "virgin-python",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<chunk.RegexpParser with 1 stages>"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser_chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "refined-cassette",
   "metadata": {},
   "source": [
    "Now, the parser will parse the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "collect-warren",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sentence' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-6e0f9df56170>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mparser_chunking\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'sentence' is not defined"
     ]
    }
   ],
   "source": [
    "parser_chunking.parse(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "celtic-fortune",
   "metadata": {},
   "source": [
    "Next, we are giving our output in the variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "interim-fabric",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'parser_chunking' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-1d330b77a72d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mOutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparser_chunking\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'parser_chunking' is not defined"
     ]
    }
   ],
   "source": [
    "Output = parser_chunking.parse(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cognitive-greensboro",
   "metadata": {},
   "source": [
    "With the help of following code, we can draw our output in the form of a tree as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "directed-rainbow",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'output' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-c083c90ed4b8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0moutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'output' is not defined"
     ]
    }
   ],
   "source": [
    "output.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "flexible-spanish",
   "metadata": {},
   "source": [
    "<img src='https://www.tutorialspoint.com/python_web_scraping/images/phrase_chunking.jpg'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decreased-kazakhstan",
   "metadata": {},
   "source": [
    "#### Bag of Word (BoW) Model Extracting and converting the Text into Numeric Form"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "natural-elephant",
   "metadata": {},
   "source": [
    "Bag of Word (BoW), a useful model in natural language processing, is basically used to extract the features from text. After extracting the features from the text, it can be used in modeling in machine learning algorithms because raw data cannot be used in ML applications.\n",
    "\n",
    "Working of BoW Model\n",
    "Initially, model extracts a vocabulary from all the words in the document. Later, using a document term matrix, it would build a model. In this way, BoW model represents the document as a bag of words only and the order or structure is discarded.\n",
    "\n",
    "Example\n",
    "\n",
    "Suppose we have the following two sentences −\n",
    "\n",
    "Sentence1 − This is an example of Bag of Words model.\n",
    "\n",
    "Sentence2 − We can extract features by using Bag of Words model.\n",
    "\n",
    "Now, by considering these two sentences, we have the following 14 distinct words −"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thorough-marijuana",
   "metadata": {},
   "source": [
    "- This\n",
    "- is\n",
    "- an\n",
    "- example\n",
    "- bag\n",
    " - of\n",
    "- words\n",
    "- model\n",
    "- we\n",
    "- can\n",
    "- extract\n",
    "- features\n",
    "- by\n",
    "- using"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "significant-failing",
   "metadata": {},
   "source": [
    "#### Building a Bag of Words Model in NLTK\n",
    "Let us look into the following Python script which will build a BoW model in NLTK.\n",
    "\n",
    "First, import the following package −\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "finite-snapshot",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "noticed-drunk",
   "metadata": {},
   "source": [
    "Next, define the set of sentences −"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "amber-fantasy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'this': 10, 'is': 7, 'an': 0, 'example': 4, 'of': 9, 'bag': 1, 'words': 13, 'model': 8, 'we': 12, 'can': 3, 'extract': 5, 'features': 6, 'by': 2, 'using': 11}\n"
     ]
    }
   ],
   "source": [
    "Sentences=['This is an example of Bag of Words model.', ' We can extract features by using Bag of Words model.']\n",
    "vector_count = CountVectorizer()\n",
    "features_text = vector_count.fit_transform(Sentences).todense()\n",
    "print(vector_count.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "julian-cookbook",
   "metadata": {},
   "source": [
    "### Python Web Scraping - Dynamic Websites\n",
    "\n",
    "Web scraping is a complex task and the complexity multiplies if the website is dynamic. According to United Nations Global Audit of Web Accessibility more than 70% of the websites are dynamic in nature and they rely on JavaScript for their functionalities.\n",
    "\n",
    "#### Dynamic Website Example\n",
    "\n",
    "Let us look at an example of a dynamic website and know about why it is difficult to scrape. Here we are going to take example of searching from a website named http://example.webscraping.com/places/default/search. But how can we say that this website is of dynamic nature? It can be judged from the output of following Python script which will try to scrape data from above mentioned webpage −"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "improved-third",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import urllib.request\n",
    "response = urllib.request.urlopen('http://example.webscraping.com/places/default/search')\n",
    "html = response.read()\n",
    "text = html.decode()\n",
    "re.findall('(.*?)',text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caroline-venezuela",
   "metadata": {},
   "source": [
    "#### Approaches for Scraping data from Dynamic Websites\n",
    "We have seen that the scraper cannot scrape the information from a dynamic website because the data is loaded dynamically with JavaScript. In such cases, we can use the following two techniques for scraping data from dynamic JavaScript dependent websites −\n",
    "\n",
    "- Reverse Engineering JavaScript\n",
    "- Rendering JavaScript\n",
    "\n",
    "if you wanto to know more please visit this.\n",
    "https://www.tutorialspoint.com/python_web_scraping/python_web_scraping_dynamic_websites.htm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seventh-performer",
   "metadata": {},
   "source": [
    "## Python Web Scraping - Form based Websites\n",
    "\n",
    "These days WWW (World Wide Web) is moving towards social media as well as usergenerated contents. So the question arises how we can access such kind of information that is beyond login screen? For this we need to deal with forms and logins.\n",
    "\n",
    "In previous chapters, we worked with HTTP GET method to request information but in this chapter we will work with HTTP POST method that pushes information to a web server for storage and analysis.\n",
    "\n",
    "if you want to know more about this topic please refer to below link.\n",
    "\n",
    "https://www.tutorialspoint.com/python_web_scraping/python_web_scraping_form_based_websites.htm\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fatal-number",
   "metadata": {},
   "source": [
    "## Python Web Scraping - Processing CAPTCHA\n",
    "\n",
    "#### What is CAPTCHA?\n",
    "The full form of CAPTCHA is Completely Automated Public Turing test to tell Computers and Humans Apart, which clearly suggests that it is a test to determine whether the user is human or not.\n",
    "\n",
    "A CAPTCHA is a distorted image which is usually not easy to detect by computer program but a human can somehow manage to understand it. Most of the websites use CAPTCHA to prevent bots from interacting.\n",
    "\n",
    "if you want to know more about it please refer to this. \n",
    "https://www.tutorialspoint.com/python_web_scraping/python_web_scraping_processing_captcha.htm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "described-salon",
   "metadata": {},
   "source": [
    "### Python Web Scraping - Testing with Scrapers\n",
    "In large web projects, automated testing of website’s backend is performed regularly but the frontend testing is skipped often. The main reason behind this is that the programming of websites is just like a net of various markup and programming languages. We can write unit test for one language but it becomes challenging if the interaction is being done in another language. That is why we must have suite of tests to make sure that our code is performing as per our expectation.\n",
    "\n",
    "if you wanto to know more about this please refer to this.\n",
    "\n",
    "https://www.tutorialspoint.com/python_web_scraping/python_web_scraping_testing_with_scrapers.htm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proved-generic",
   "metadata": {},
   "source": [
    "### Useful Books on Python Web Scraping\n",
    "<img src = 'https://images-na.ssl-images-amazon.com/images/I/51u9mDi83gL._SX404_BO1,204,203,200_.jpg'>\n",
    "\n",
    "<img src = 'https://images-na.ssl-images-amazon.com/images/I/51KgwVgNVOL._SX379_BO1,204,203,200_.jpg'>\n",
    "\n",
    "<img src='https://images-na.ssl-images-amazon.com/images/I/61X0QVbrUvL._SX404_BO1,204,203,200_.jpg'>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "liable-shannon",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
